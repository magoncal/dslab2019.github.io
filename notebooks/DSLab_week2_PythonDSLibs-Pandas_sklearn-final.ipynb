{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting familiar with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Pandas \n",
    "# there are several ways to change a column in a dataframe\n",
    "# A short intro to pandas https://pandas.pydata.org/pandas-docs/stable/10min.html\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First step: make the dataframe\n",
    "dates = pd.date_range('20130101', '20140101') #366\n",
    "data = pd.DataFrame(np.random.randn(366,4), index=dates, columns=list('ABCD'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Inspect the dataframe with the following commands: head(), tail(), describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2:  The index is a time series, and pandas has a build-in command for re-sampling dataframes (documentation: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html).  Use resample to get the median every 2 days and save this as a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Solution: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Inspect the new dataframe to see the difference in size compared to the inital dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Solution: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4:  Write your new dataframe to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Solution: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5: Merge the two dataframes. There are several ways to do this, see also https://pandas.pydata.org/pandas-docs/stable/merging.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6: There are several ways to perform actions on the dataframe columns. The dataframe has several columns containing negative values. For this exercise, find these negative values on a column, and create a new column with their absolute value, using a list comprehension, and after this, using a lambda function. You can use the magic timeit to see if there is a difference between these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "# method 1: list comprehension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "# method 2: lambda function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning using scikit-learn - Classification of MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Download the digit ('MNIST original') dataset from  mldata.org, which is a public repository for machine learning data. Divide the data into training and testing. Please use 1/7 for training and the rest for testing. \n",
    "\n",
    "Hint: The sklearn.datasets package is able to directly download data sets from the repository using the function sklearn.datasets.fetch_mldata. Generate the training and testing set by importing train_test_split from sklearn.model_selection\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "import sklearn \n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "# Download the MNIST original dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the images into training and testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2.2: The optimal performance of many machine learning algorithms is affected by scale. Typically, you need to scale the features in your data before applying any algorithm. Normalize the data and plot some random images from the dataset.  \n",
    "\n",
    "Hint: Use StandardScaler from sklearn.preprocessing to help you standardize the dataset’s features onto unit scale (mean = 0 and variance = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only\n",
    "\n",
    "# Apply transform to both the training set and the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution (Visualization)\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Logistic regression is one of the simplest linear classification algorithms. Fit a logistic regression model to the training images. Compute the accuracy of the classifier on the test images, and the time needed to train the model.¶\n",
    "\n",
    "Hint: Use LogisticRegression from sklearn.linear_model. To increase speed, change the default solver to 'lbfgs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "from time import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tic = time()\n",
    "# Fit a linear regression model\n",
    "\n",
    "# Compute the classification score\n",
    "\n",
    "toc = time()\n",
    "print('The total time is %s seconds ' % (toc-tic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Apply Principle Component Analysis (PCA) to the training signals by keeping only (a) 25%, (b) 75%, and (c) 95% of the energy. For each of the three cases, output the number of the required principle components.Then, plot the Cumulative Explained Variance over PCA. Finally, choose a random image from the dataset, and show its approximation with the PCA components. \n",
    "\n",
    "Hint: For computing the Cumulative Explained Variance over PCA use:\n",
    "```\n",
    "pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Fit a PCA model\n",
    "\n",
    "# Compute the number of PCA components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the Cumulative Explained Variance over PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose a random image from the dataset, and show its approximation with the PCA components\n",
    "\n",
    "plt.figure(figsize=(8,4));\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, 2, 1);\n",
    "\n",
    "# Approximation\n",
    "plt.subplot(1, 2, 2);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5: Fit a logistic regression model to the approximation of the training images with 95% of explained variance. Compute the accuracy of the classifier and the time needed to train the model. Compare it to the one obtained in 2.3. What do you observe? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "tic = time()\n",
    "\n",
    "# Fit a logistic regression model on the PCA coefficients\n",
    "\n",
    "toc = time()\n",
    "\n",
    "print('The total time is %s seconds' % (toc-tic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning with sklearn.cluster.KMeans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 3.1: Generate a set of 6 isotropic Gaussian blobs, with 1000 samples each. Each sample should have 60 features. \n",
    "\n",
    "Hint: Use the sklearn.datasets.make_blobs to generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# Generate the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 3.2: Apply PCA to the generated data. Store the first two principle components and their cluster index to a new dataframe.  Visualize the 6 blobs based only on these two components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# Fit PCA to the data\n",
    "\n",
    "# Generate a new dataframe and store the first two Principle Components and the true cluster index\n",
    "\n",
    "# Vizualize the data by plotting their representation on the two Principle Components (x and y axis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Set the number of clusters to 6 and apply Kmeans clustering to the data. Compute the accuracy score between the true labels and the ones estimated by the Kmeans algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit a Kmean model to the data\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Uncomment this part to compute the accuracy score\n",
    "#  y_true: the true cluster index\n",
    "#  y_kmeans: the cluster index assigned by Kmeans\n",
    "\n",
    "\"\"\"\n",
    "labels = np.zeros_like(y_true)\n",
    "for i in range(6):\n",
    "    mask = (y_kmeans == i)\n",
    "    labels[mask] = mode(y_true[mask])[0]\n",
    "    \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, labels)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4: Do the same by clustering the data using only the first 2 principle components. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# Fit a Kmeans model to the first 2 PCA coefficients of the data\n",
    "\n",
    "# Uncomment this part to compute the accuracy score\n",
    "# y_true: the true cluster index\n",
    "# y_kmeans: the cluster index assigned by Kmeans\n",
    "\n",
    "\"\"\"\n",
    "labels = np.zeros_like(y_true)\n",
    "for i in range(6):\n",
    "    mask = (y_kmeans == i)\n",
    "    labels[mask] = mode(y_true[mask])[0]\n",
    "    \n",
    "accuracy_score(y_true, labels)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
